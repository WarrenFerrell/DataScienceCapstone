---
title: "Capstone Milestone Report"
author: "Warren T Ferrell Jewell"
date: "November 16, 2016"
output:
  html_document: default
  pdf_document: default
---

## About the Data
pulled list of most common english words from https://github.com/first20hours/google-10000-english


```{r setup, include=FALSE}
#Sys.setenv("R_COMPILE_PKGS" = 1) #compile all packages on install
#enableJIT(3)  # just in time compiling (compile each function the first time it is run
#rm(list=ls())
mainPath <- "C:/Users/warre/Dropbox/GitHub/CAPSTONE/"
setwd(mainPath)
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message=FALSE, warning=FALSE)
library(tm); library(ggplot2); library(dplyr); library(compiler); library(methods)
library(foreach); library(doParallel); library(microbenchmark); library(fastmatch)
source(paste0(mainPath, "scripts/cleanLine.R"))
source(paste0(mainPath, "scripts/proftable.R"))
scriptPath <- paste0(mainPath, 'scripts/')
objPath <- paste0(mainPath, 'objects/')
```

```{r variables}
xChars = 1E4 # work with subset of data from each set
vocabulary = 20000
cacheObj = FALSE
minFreq <- 2
maxGram <- 10
maxSize <-  1000  # limit of tree in MB : memory.limit() - mem_used() 
```


##Common Terms
Using on the top 20000 terms (minimum vocabulary for an adult)

```{r topTerms, echo = FALSE}
load(file = "objects/termFreq")
topTerms <- termFreq[1:vocabulary]
#rm(termFreq)
includedTerms <- c(names(topTerms), "\"", ".", "?", "!", ",",";", ":", "&") # add punctuation to terms to keep
```

## Load Data
Generate trees by traversing each line in file. Use a environment of environments (a hash table) to dramatically reduce the size of the tree. Each term has a list
that points to terms that have been seen following that term. The final term in the
gram is stored in the list with a frequency variable that is used to rank which
is the most likely gram during prediction. Because this method doesn't grab the 
frequency of every single gram (something that requires more memory than a 16GB RAM
laptop can provide) it sets a maximum allowable memory for the tree and then
trims all all grams that don't meet a set minimum frequency. This method can lose
low frequency grams with the highest frequency that could be lost being equal to
the number of times that the tree is trimed.


```{r loadData, echo=FALSE}
trainPath <- paste0(mainPath, 'data/en_US/',xChars,'/')
objPre <- paste0(objPath, 'refs/inPlace/', xChars, 'Vocab', vocabulary, 'MaxSize', maxSize)

source(paste0(scriptPath, 'refs/source.R'))

formals(ls)$sorted <- FALSE #slowest part of ls

#source(paste0(scriptPath, 'refs/compile.R'))
enableJIT(3)

Rprof('treeMake.out')

system.time({gramTree <- readRefData(trainPath, includedTerms, maxGram, maxSize, xChars, FALSE)})

Rprof(NULL)

#system.time(save(gramTree, file = fileName)) 

# summaryRprof('treeMake.out')
# proftable('treeMake.out')

```

## Predict
With our generated tree we predict some common phrases

```{r predictFunction, echo=FALSE}

source(paste0(scriptPath, 'env2list.R'))
nGramTree.sort <- compiler::cmpfun( nGramTree.sort )
env2list <- compiler::cmpfun( env2list )




fileName <- paste0(objPre, 'gList', 'MinFreq', minFreq) 
Rprof('listMake.out')
system.time({gList <- env2list(gramTree$tree)}) #faster and smaller to change to List and then save
system.time({save(gList, file = fileName)})
Rprof(NULL)
Rprof('listSort.out')
system.time({gList <- nGramTree.sort(gList)})
system.time({save(gList, file = fileName)})
Rprof(NULL)
object_size(gramTree)
object_size(gList)
```

```{r test}
source(paste0(scriptPath, 'refs/refPredRaw.R'))
formals(nGramTree.predict)$k <- maxGram
formals(nGramTree.predict)$commonTerms <- includedTerms
treeSearch <- compiler::cmpfun( treeSearch )
nGramTree.predict <- compiler::cmpfun( nGramTree.predict )


# nGramTree.predict( gramTree, 'the', maxGram )
# nGramTree.predict( gramTree, 'in', maxGram )
# nGramTree.predict( gramTree, 'of', maxGram )
# nGramTree.predict( gramTree, 'case of', maxGram )
# nGramTree.predict( gramTree, 'a case of', maxGram )

```


```{r}
#quiz <- c("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 
    "You're the reason why I smile everyday. Can you follow me please? It would mean the",
    "Hey sunshine, can you follow me and make me the",
    "Very early observations on the Bills game: Offense still struggling but the",
    "Go on a romantic date at the",
    "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my",
    "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some",
    "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little",
    "Be grateful for the good times and keep the faith during the",
    "If this isn't the cutest thing you've ever seen, then you must be")

```


```{r predict}

#lapply(quiz, function(x) nGramTree.predict( gramTree, x, maxGram ) )


```


## Thoughts
Need to remove grams that resulted from removing uncommon terms from the dataset. 


## references
https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/

http://stackoverflow.com/questions/13614399/how-can-i-use-attr-with-lapply

http://stackoverflow.com/questions/32997201/how-to-store-sparsity-and-maximum-term-length-of-a-term-document-matrix-from-tm

http://www.economist.com/blogs/johnson/2013/05/vocabulary-size

http://stackoverflow.com/questions/31527345/how-to-add-unequal-length-named-vectors-in-r
